{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = \"traffic-signs-data/train.p\"\n",
    "testing_file = \"traffic-signs-data/test.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "10\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(set(y_train.reshape(y_train.shape[0])))\n",
    "n_classes = len(set(y_train.reshape(y_train.shape[0])))\n",
    "print(n_classes)\n",
    "\n",
    "y_train = y_train.reshape(y_train.shape[0])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 2D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n",
    "Complete the basic data summary below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Replace each question mark with the appropriate value.\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc.\n",
    "\n",
    "The [Matplotlib](http://matplotlib.org/) [examples](http://matplotlib.org/examples/index.html) and [gallery](http://matplotlib.org/gallery.html) pages are a great resource for doing visualizations in Python.\n",
    "\n",
    "**NOTE:** It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Data exploration visualization goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# store the label, sign name, and sample image of each traffic sign\n",
    "sample_data = {}\n",
    "\n",
    "# plot the the number of samples for each traffic sign\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(y_train, bins=n_classes)\n",
    "plt.xlabel('class labels')\n",
    "plt.ylabel('number of samples')\n",
    "plt.title('Class Sample Distribution in Training Set')\n",
    "plt.show()\n",
    "\n",
    "# get the sign names\n",
    "import pandas as pd\n",
    "df = pd.read_csv('signnames.csv')\n",
    "\n",
    "# show an example of each class of traffic sign\n",
    "from textwrap import wrap\n",
    "from collections import Counter\n",
    "label_counter = Counter(y_train)\n",
    "fig = plt.figure(figsize=(10,15))\n",
    "idx = 0 #image index\n",
    "for label,samples in label_counter.items():\n",
    "    image = X_train[idx].squeeze()\n",
    "    ax = fig.add_subplot(8,8,label+1)\n",
    "    ax.imshow(image)\n",
    "    title = df.iloc[label,1]\n",
    "    #wrap the title\n",
    "    ax.set_title(\"\\n\".join(wrap(title, 12)))   \n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    idx += samples\n",
    "    sample_data[label] = (title,image)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these.\n",
    "\n",
    "**NOTE:** The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Preprocess the data here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "\n",
    "# determine the number of samples per each class\n",
    "avg_sample_count = n_train / n_classes\n",
    "\n",
    "# create additional data for under represented classes by rotating the images\n",
    "additional_samples = np.empty([0,32,32,3])\n",
    "additional_labels = np.empty([0])\n",
    "current = 0\n",
    "for label,no_of_samples in label_counter.items():\n",
    "    # if number of samples for the class is less than average / 2, create more samples by rotating images\n",
    "    print(\"process label\", label, \"with samples\", no_of_samples)\n",
    "    if no_of_samples < avg_sample_count / 2:\n",
    "        begin = current\n",
    "        end = current + no_of_samples\n",
    "        print(\"adding\", 3*(end-begin), \"samples\", \"for label\", label)\n",
    "        r1 = rotate(X_train[begin:end], axes=(1,2), angle=15, reshape=False)\n",
    "        r2 = rotate(X_train[begin:end], axes=(1,2), angle=30, reshape=False)\n",
    "        r3 = rotate(X_train[begin:end], axes=(1,2), angle=-15, reshape=False)\n",
    "        additional_samples = np.concatenate([additional_samples, r1, r2, r3])\n",
    "        additional_labels = np.concatenate([additional_labels, y_train[begin:end], y_train[begin:end], y_train[begin:end]])\n",
    "    current = current + no_of_samples\n",
    "\n",
    "X_train = np.concatenate([X_train, additional_samples])\n",
    "y_train = np.concatenate([y_train, additional_labels])\n",
    "\n",
    "# plot the new class distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(y_train, bins=n_classes)\n",
    "plt.xlabel('class labels')\n",
    "plt.ylabel('number of samples')\n",
    "plt.title('New Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# zero center the data\n",
    "X_train = X_train - np.mean(X_train)\n",
    "X_test = X_test - np.mean(X_test)\n",
    "\n",
    "# normalize the data\n",
    "X_train = X_train / np.std(X_train)\n",
    "X_test = X_test / np.std(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe how you preprocessed the data. Why did you choose that technique?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "We apply mean substraction by substracting the mean across every feature. We also normalize the data by dividing it by the standard deviation. This common preprocessing technique can rearrange data to have similar scale near the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Generate data additional data (OPTIONAL!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. **Optional**: If you generated additional data, how did you generate the data? Why did you generate the data? What are the differences in the new dataset (with generated data) from the original dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "Use 20% of the training data for validation. Use remaining 80% of the training data for training.\n",
    "\n",
    "For classes of traffic sign that are under represented in the training data, I created additional data by rotating the images. The additional data will result in more robust learning and protect against simliar deformations in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "KEEP_PROB = 0.5\n",
    "\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x, keep_prob):\n",
    "    # Hyperparameters\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6\n",
    "    w1 = tf.Variable(tf.truncated_normal(shape=(5,5,3,6), mean = mu, stddev = sigma))\n",
    "    b1 = tf.Variable(tf.zeros(6))\n",
    "    conv1 = tf.nn.conv2d(x, w1, strides=[1,1,1,1], padding='VALID')\n",
    "    conv1 = tf.nn.bias_add(conv1, b1)\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    #Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    w2 = tf.Variable(tf.truncated_normal(shape=(5,5,6,16), mean = mu, stddev = sigma))\n",
    "    b2 = tf.Variable(tf.zeros(16))\n",
    "    conv2 = tf.nn.conv2d(conv1, w2, strides=[1,1,1,1], padding='VALID')\n",
    "    conv2 = tf.nn.bias_add(conv2, b2)    \n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    conv2 = flatten(conv2)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    w3 = tf.Variable(tf.truncated_normal(shape=(400,120), mean = mu, stddev = sigma))\n",
    "    b3 = tf.Variable(tf.zeros(120))\n",
    "    fc1 = tf.add(tf.matmul(conv2, w3), b3)\n",
    "    fc1 = tf.nn.relu(fc1)    \n",
    "    \n",
    "    # Dropout 1\n",
    "    drop1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    w4 = tf.Variable(tf.truncated_normal(shape=(120,84), mean = mu, stddev = sigma))\n",
    "    b4 = tf.Variable(tf.zeros(84))\n",
    "    fc2 = tf.add(tf.matmul(drop1,w4), b4)\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Dropout 2\n",
    "    drop2 = tf.nn.dropout(fc2, keep_prob)    \n",
    "    \n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    w5 = tf.Variable(tf.truncated_normal(shape=(84,n_classes), mean = mu, stddev = sigma))\n",
    "    b5 = tf.Variable(tf.zeros([n_classes]))\n",
    "    logits = tf.add(tf.matmul(drop2,w5), b5)\n",
    "    \n",
    "    return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "- A convolutional layer with a 5x5 filter, 1x1 stride, VALID padding, and depth of 6. The output is 28x28x6\n",
    "- A relu activation\n",
    "- A pooling layer with 2x2 kernel and 2x2 stride. The output is 14x14x6\n",
    "- A convolutional layer with a 5x5 filter, 1x1 stride, VALID padding, and depth of 16. The output is 10x10x6\n",
    "- A relu activation \n",
    "- A pooling layer with 2x2 kernel and 2x2 stride. The output is 5x5x6\n",
    "- flatten the output from previous layer into a vector 5x5x16 = 400\n",
    "- A fully connected layer with width of 120\n",
    "- A dropout layer with keep probablity of 0.5\n",
    "- Another fully connected layer with width of 84\n",
    "- A dropout layer with keep probablity of 0.5\n",
    "- An output layer with 43 width (number of classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ... Validation Accuracy = 0.448\n",
      "EPOCH 2 ... Validation Accuracy = 0.496\n",
      "EPOCH 3 ... Validation Accuracy = 0.517\n",
      "EPOCH 4 ... Validation Accuracy = 0.532\n",
      "EPOCH 5 ... Validation Accuracy = 0.539\n",
      "EPOCH 6 ... Validation Accuracy = 0.565\n",
      "EPOCH 7 ... Validation Accuracy = 0.571\n",
      "EPOCH 8 ... Validation Accuracy = 0.569\n",
      "EPOCH 9 ... Validation Accuracy = 0.586\n",
      "EPOCH 10 ... Validation Accuracy = 0.581\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [32] vs. [32,10]\n\t [[Node: Equal_1 = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax_2, ArgMax_3)]]\n\nCaused by op 'Equal_1', defined at:\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-0cf2cc59f395>\", line 19, in <module>\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 728, in equal\n    result = _op_def_lib.apply_op(\"Equal\", x=x, y=y, name=name)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [32] vs. [32,10]\n\t [[Node: Equal_1 = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax_2, ArgMax_3)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [32] vs. [32,10]\n\t [[Node: Equal_1 = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax_2, ArgMax_3)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0cf2cc59f395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH {} ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Validation Accuracy = {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy = {:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0cf2cc59f395>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(X_data, y_data)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_operation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_accuracy\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [32] vs. [32,10]\n\t [[Node: Equal_1 = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax_2, ArgMax_3)]]\n\nCaused by op 'Equal_1', defined at:\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-0cf2cc59f395>\", line 19, in <module>\n    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 728, in equal\n    result = _op_def_lib.apply_op(\"Equal\", x=x, y=y, name=name)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/vincechan/anaconda/envs/carnd-term1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [32] vs. [32,10]\n\t [[Node: Equal_1 = Equal[T=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ArgMax_2, ArgMax_3)]]\n"
     ]
    }
   ],
   "source": [
    "### Train your model here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "one_hot_y = tf.one_hot(y, n_classes)\n",
    "\n",
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x, keep_prob)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "\n",
    "#train the model\n",
    "accuracy = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: KEEP_PROB})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        accuracy.append(validation_accuracy)\n",
    "        print(\"EPOCH {} ...\".format(i+1), \"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        \n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    \n",
    "    saver.save(sess, 'lenet')\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "#plot accuracy over epochs\n",
    "plt.plot(range(len(accuracy)), accuracy)\n",
    "plt.ylim([0.7,1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the model with test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I used AdamOptimizer with a learning rate of 0.001. I had experimented with lowering learning rate, it requires a much larger epochs and as a result, much longer training time with only marginal improvement.\n",
    "\n",
    "I used 100 epochs. I had experiemented with much larger epochs (>1000) combined with lower learning rate, it resulted with some marginal validation accuracy, but much longer training time. \n",
    "\n",
    "I used two drop out layers with 0.5 keep probability, one after each fully connected layer. The dropout layers are added to prevent overfitting.\n",
    "\n",
    "I used a batch size of 32. I picked 32 mainly due to the limitation of my machine which only has a 1GB of GPU memory. However, I did experiment with much larger batch size (up to 4096) when I used Amazon EC2 instance. I found that it improved the training time greatly, but didn't improve validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem? It may have been a process of trial and error, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think this is suitable for the current problem._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The entire solution is the result of many trial and errors. \n",
    "\n",
    "For my first attempt, I simply adapted the LeNet Lab to this problem. I had to adjust the input from depth of 1 (grayscale in MINIST) to depth of 3 (color in traffic signs). I also had to adjust the final layer to n_classes (43) from 10. This alone got the validation accuracy to 96%. \n",
    "\n",
    "Next, I did many experiements by adjusting various hyper parameters on the existing model. When I used 500 epochs and I noticed the validation accuracy started to go down after 200 epochs. I suspected the model overshot optimum. \n",
    "I adjusted the learning rate from 0.001 to 0.0001. After this adjustment, with 0.0001 learning rate and 500 epochs, the validation accuracy increased to 99%.\n",
    "\n",
    "<img src=\"500epochs.png\"/>\n",
    "\n",
    "\n",
    "Next I experiemented changing the architecture. The first idea I had was related to the padding. The first conv layer from LeNet lab was cutting off the edges. The first layer has an input of 32x32 and output of 28x28. This cutoff was fine for MINIST in the LeNet lab where the original images was smaller and had zero padded around the edges. I thought if there are useful features near the edges in the traffic sign images, they would get cut off. Based on that idea, I updated the first conv layer to use 'SAME' instead of 'VALID' padding. This will give an output of 32x32 after the first conv layer. However, that didn't produce any noticeable improvement. I also experimented with different depths in the conv layers without noticeable improvement.\n",
    "\n",
    "Next, I experiemented with the batch size. On my local machine, my GPU only has 1GB memory, and I couldn't use anything bigger than. To get around that limiation, I started using an Amazon EC2 gp2 instance. On the Amazon instance, I used 4096 as the batch size. The training time was a lot faster, but the accuracy wasn't improving.\n",
    "\n",
    "Next, reasonably satifitied with the 99% valiation accuracy, I gave the test data a shot. The test accuracy was ~92%, quite a big gap from the validation accuracy. This clearly indicated overfitting. Also, this could also be a result of deformed images in the test set that the trained model couldn't predict.\n",
    "\n",
    "Equipped with what I learnt from the last experiment. Next, I added preprocessing. I normalized the data. I also generated additional training data. I also noticed there are many classes that are very under represented in the training data. I generated the additional data for those classes by rotating the images. I also added two dropout layers, one after each fully connected layer to combat overfitting. With the new pipeline, I could increase the learning rate, decrease the epochs and speed up the training, and achieved a 99.2% validation accuracy and 95% test accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "extras = [\n",
    "    { \"file\": \"img1.png\", \"image\": None, \"pred_label\": None, \"label\": 13, \"sign\": \"Yield\" },\n",
    "    { \"file\": \"img2.png\", \"image\": None, \"pred_label\": None, \"label\": 14, \"sign\": \"Stop\" },\n",
    "    { \"file\": \"img3.png\", \"image\": None, \"pred_label\": None, \"label\": 17, \"sign\": \"No Entry\" },\n",
    "    { \"file\": \"img4.png\", \"image\": None, \"pred_label\": None, \"label\": 18, \"sign\": \"General Caution\" },\n",
    "    { \"file\": \"img5.png\", \"image\": None, \"pred_label\": None, \"label\": 28, \"sign\": \"Children Crossing\" },\n",
    "    { \"file\": \"img6.png\", \"image\": None, \"pred_label\": None, \"label\": -1, \"sign\": \"Railroad Crossing\" },    \n",
    "];\n",
    "\n",
    "for extra in extras:\n",
    "    image = mpimg.imread(\"extra/\" + extra[\"file\"])\n",
    "    image = image[:,:,:3]\n",
    "    extra[\"image\"] = cv2.resize(image, (32,32))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It could be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import tensorflow as tf\n",
    "\n",
    "# run predictions\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))   \n",
    "    pred = sess.run(tf.argmax(logits, 1), feed_dict = { x: [e[\"image\"] for e in extras], keep_prob: 1.0})\n",
    "\n",
    "# store predictions in extras \n",
    "for i in range(len(pred)):\n",
    "    extras[i][\"pred_label\"] = pred[i]\n",
    "\n",
    "# plot result\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "for idx, extra in enumerate(extras):\n",
    "    ax = fig.add_subplot(len(extras), 3, idx * 3 + 1)\n",
    "    ax.imshow(extra[\"image\"])\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    correctness = \"CORRECT\" if extra[\"label\"] == extra[\"pred_label\"] else \"WRONG\"\n",
    "    ax.text(40,20,\"Prediction: {0}\\nPredicted Label: {1}\\nCorrect Label: {2}\".format(correctness, extra[\"pred_label\"], extra[\"label\"]))\n",
    "    \n",
    "    ax = fig.add_subplot(len(extras), 3, idx * 3 + 2)\n",
    "    ax.imshow(sample_data[extra[\"pred_label\"]][1])\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.text(40,20,\"Predicted\\n{0}\".format(sample_data[extra[\"pred_label\"]][0]))\n",
    "    \n",
    "    ax = fig.add_subplot(len(extras), 3, idx * 3 + 3)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())    \n",
    "    correct_label = extra[\"label\"]\n",
    "    if correct_label == -1:\n",
    "        ax.imshow(np.zeros((32,32,3)))\n",
    "        ax.text(40,20,\"Actual\\n{0}\\n(Not in set)\".format(extra[\"sign\"]))\n",
    "    else:\n",
    "        ax.imshow(sample_data[extra[\"label\"]][1])\n",
    "        ax.text(40,20,\"Actual\\n{0}\".format(sample_data[extra[\"label\"]][0]))\n",
    "plt.tight_layout()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are two images that model did not predict correctly. \n",
    "\n",
    "Children crossing. \n",
    "This sign is in the training set, but there are quite a few differences with the extra image that could throw off the model. First, The shape of the sign in the training set is triangle, with thick red border. The shape of the sign in the extra image is pentagon, and it's entirely yellow with much thinner border. Also, the shape of the two people in the signs are different. Moreover, the extra image has a sentence at the bottom that's absent in the training set.\n",
    "\n",
    "Railroad crossing. \n",
    "This image is not in the training set, so the model could never have predicted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures when compared to testing on the dataset? The simplest way to do this check the accuracy of the predictions. For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate._\n",
    "\n",
    "_**NOTE:** You could check the accuracy manually by using `signnames.csv` (same directory). This file has a mapping from the class id (0-42) to the corresponding sign name. So, you could take the class id the model outputs, lookup the name in `signnames.csv` and see if it matches the sign from the image._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "On the new images, the accuracy is 66.7%. This is not nearly as good as the 95% test accuracy. This is partially due to the railroad crossing image that's not in the training set, and the differences in the children crossing image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "import tensorflow as tf\n",
    "\n",
    "# run top_k on softmax\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))   \n",
    "    topk = sess.run(tf.nn.top_k(tf.nn.softmax(logits), k=5), feed_dict = { x: [e[\"image\"] for e in extras], keep_prob: 1.0})\n",
    "# print(topk)\n",
    "\n",
    "# store predictions to plot later\n",
    "for i in range(len(extras)):\n",
    "    extras[i][\"top1img\"] = sample_data[topk[1][i][0]][1]\n",
    "    extras[i][\"top1pct\"] = round(topk[0][i][0] * 100, 2)\n",
    "    extras[i][\"top2img\"] = sample_data[topk[1][i][1]][1]\n",
    "    extras[i][\"top2pct\"] = round(topk[0][i][1] * 100, 2)\n",
    "    extras[i][\"top3img\"] = sample_data[topk[1][i][2]][1]\n",
    "    extras[i][\"top3pct\"] = round(topk[0][i][2] * 100, 2)\n",
    "    \n",
    "#for e in extras:\n",
    "#    print(e[\"topk\"], e[\"topkpct\"])\n",
    "\n",
    "# plot result\n",
    "fig = plt.figure(figsize = (10,6))            \n",
    "for idx, extra in enumerate(extras):\n",
    "    ax = fig.add_subplot(len(extras), 4, idx * 4 + 1)\n",
    "    ax.imshow(extra[\"image\"])\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    correctness = \"CORRECT\" if extra[\"label\"] == extra[\"pred_label\"] else \"WRONG\"\n",
    "    ax.text(40,20,\"{0}\\nPredicted : {1}\\nCorrect : {2}\".format(correctness, extra[\"pred_label\"], extra[\"label\"]))\n",
    "    \n",
    "    ax = fig.add_subplot(len(extras), 4, idx * 4 + 2)\n",
    "    ax.imshow(extra[\"top1img\"])\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.text(40,20,\"Percentage\\n{0}\".format(extra[\"top1pct\"]))\n",
    "    \n",
    "    ax = fig.add_subplot(len(extras), 4, idx * 4 + 3)\n",
    "    ax.imshow(extra[\"top2img\"])\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.text(40,20,\"Percentage\\n{0}\".format(extra[\"top2pct\"]))\n",
    "    \n",
    "    ax = fig.add_subplot(len(extras), 4, idx * 4 + 4)\n",
    "    ax.imshow(extra[\"top3img\"])\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.text(40,20,\"Percentage\\n{0}\".format(extra[\"top3pct\"]))\n",
    "plt.tight_layout()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n",
    "\n",
    "`tf.nn.top_k` will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "\n",
    "Take this numpy array as an example:\n",
    "\n",
    "```\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "```\n",
    "\n",
    "Running it through `sess.run(tf.nn.top_k(tf.constant(a), k=3))` produces:\n",
    "\n",
    "```\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "```\n",
    "\n",
    "Looking just at the first row we get `[ 0.34763842,  0.24879643,  0.12789202]`, you can confirm these are the 3 largest probabilities in `a`. You'll also notice `[3, 0, 5]` are the corresponding indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The model predicted correctly with 100% certainty on 3 images!\n",
    "\n",
    "The model predicted the stop sign correctly at 99.32%. This is a bit less cerain than the other 3 correctly predicted image. This is possibly due to the background in the stop sign in the extra image. It has a much busier background compared to other images.\n",
    "\n",
    "The model predicated the childred crossing sign incorrectly with a low certainty at 50.23%. This is a local sign that has many differences with the sign in the training set, so the low certainty makes sense. Interestingly, the next two highest probable prediction didn't include the correct sign. I ran top_k to include all predictions and found that the correct result came in at number 12 with 0.7% certainty.\n",
    "\n",
    "The model predicated the railroad crossing sign incorrectly with a high certainly at 94.77%. This image is not in the training data so the model could never have predicatd it correctly. The model predicated it as the 30 km/h speed limit sign. There are some similarities between both signs. They both have have circular border. They both have two  characters in the middle. However, they look quite different visually. I am surprised the model predicated with such high certainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
